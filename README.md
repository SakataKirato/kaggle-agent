# Kaggle Tabular Agent

ãƒ†ãƒ¼ãƒ–ãƒ«ãƒ‡ãƒ¼ã‚¿å‘ã‘Kaggleã‚³ãƒ³ãƒšã‚’è‡ªå‹•ã§è§£ãAIã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã€‚

## ğŸš€ Google Colabã§ã®å®Ÿè¡Œæ–¹æ³•

### 1. ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—

```python
# ãƒªãƒã‚¸ãƒˆãƒªã‚’ã‚¯ãƒ­ãƒ¼ãƒ³
!git clone https://github.com/YOUR_USERNAME/kaggle-agent.git
%cd kaggle-agent

# ä¾å­˜é–¢ä¿‚ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
!pip install -q llama-cpp-python pandas numpy lightgbm xgboost catboost scikit-learn

# llama-cpp-pythonã‚’GPUå¯¾å¿œã§ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ï¼ˆæ¨å¥¨ï¼‰
!CMAKE_ARGS="-DGGML_CUDA=on" pip install llama-cpp-python --force-reinstall --no-cache-dir
```

### 2. ãƒ¢ãƒ‡ãƒ«ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰

```python
!pip install -q huggingface_hub

from huggingface_hub import hf_hub_download

# ãƒ†ã‚­ã‚¹ãƒˆç†è§£ç”¨ãƒ¢ãƒ‡ãƒ«ï¼ˆè»½é‡ï¼‰
hf_hub_download(
    repo_id="unsloth/Llama-3.2-3B-Instruct-GGUF",
    filename="Llama-3.2-3B-Instruct-Q4_K_M.gguf",
    local_dir="./models"
)

# ã‚³ãƒ¼ãƒ‰ç”Ÿæˆç”¨ãƒ¢ãƒ‡ãƒ«ï¼ˆMoEã€L4ã§å‹•ä½œå¯èƒ½ï¼‰
hf_hub_download(
    repo_id="unsloth/Qwen3-Coder-30B-A3B-Instruct-GGUF",
    filename="Qwen3-Coder-30B-A3B-Instruct-Q4_K_M.gguf",
    local_dir="./models"
)
```

### 3. Kaggleãƒ‡ãƒ¼ã‚¿ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰

```python
# Kaggleèªè¨¼è¨­å®š
from google.colab import files
files.upload()  # kaggle.json ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰

!mkdir -p ~/.kaggle
!mv kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json

# ã‚³ãƒ³ãƒšãƒ‡ãƒ¼ã‚¿ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ï¼ˆä¾‹: Titanicï¼‰
!kaggle competitions download -c titanic -p ./data/titanic
!unzip -o ./data/titanic/titanic.zip -d ./data/titanic
```

### 4. ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆå®Ÿè¡Œ

```python
!python agent.py --competition ./data/titanic --max-iterations 5
```

ã¾ãŸã¯ã€Pythonã‹ã‚‰ç›´æ¥å®Ÿè¡Œï¼š

```python
from agent import KaggleTabularAgent, AgentConfig

config = AgentConfig(
    competition_dir="./data/titanic",
    text_model_path="models/Llama-3.2-3B-Instruct-Q4_K_M.gguf",
    code_model_path="models/Qwen3-Coder-30B-A3B-Instruct-Q4_K_M.gguf",
    max_improvement_iterations=5
)

agent = KaggleTabularAgent(config)
result = agent.run()
print(f"Final Score: {result['final_score']}")
```

## ğŸ“‹ å¿…è¦ãªGPU

| ãƒ¢ãƒ‡ãƒ« | æœ€å°VRAM |
|-------|---------|
| Llama-3.2-3B (Q4) | ~3GB |
| Qwen3-Coder-30B-A3B (Q4) | ~18GB |

**æ¨å¥¨**: Colab Pro/Pro+ ã® **L4 GPU** (24GB)

> âš ï¸ ç„¡æ–™ç‰ˆColab (T4: 16GB) ã§ã¯ Qwen3-Coder-30B-A3B ãŒå‹•ä½œã—ãªã„å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚

## ğŸ“ ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæ§‹é€ 

```
kaggle-agent/
â”œâ”€â”€ agent.py              # ãƒ¡ã‚¤ãƒ³ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ core/
â”‚   â”œâ”€â”€ llm.py            # LLMç®¡ç†
â”‚   â”œâ”€â”€ executor.py       # ã‚³ãƒ¼ãƒ‰å®Ÿè¡Œ
â”‚   â””â”€â”€ memory.py         # ãƒ¡ãƒ¢ãƒªç®¡ç†
â”œâ”€â”€ phases/
â”‚   â”œâ”€â”€ understanding.py  # ã‚³ãƒ³ãƒšç†è§£
â”‚   â”œâ”€â”€ eda.py            # EDA
â”‚   â”œâ”€â”€ feature_engineering.py
â”‚   â”œâ”€â”€ modeling.py
â”‚   â””â”€â”€ ensemble.py
â””â”€â”€ models/               # GGUFãƒ¢ãƒ‡ãƒ«é…ç½®
```

## ğŸ”§ ã‚ªãƒ—ã‚·ãƒ§ãƒ³

```bash
python agent.py \
  --competition ./data/titanic \
  --competition-name titanic \
  --text-model ./models/Llama-3.2-3B-Instruct-Q4_K_M.gguf \
  --code-model ./models/Qwen3-Coder-30B-A3B-Q4_K_M.gguf \
  --max-iterations 10 \
  --target-score 0.85
```

| ã‚ªãƒ—ã‚·ãƒ§ãƒ³ | èª¬æ˜ |
|-----------|------|
| `--competition` | ãƒ‡ãƒ¼ã‚¿ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ãƒ‘ã‚¹ï¼ˆå¿…é ˆï¼‰ |
| `--competition-name` | Kaggleã‚³ãƒ³ãƒšåï¼ˆAPIã‹ã‚‰æƒ…å ±å–å¾—ï¼‰ |
| `--max-iterations` | æ”¹å–„ãƒ«ãƒ¼ãƒ—ã®æœ€å¤§å›æ•° |
| `--target-score` | ç›®æ¨™ã‚¹ã‚³ã‚¢ï¼ˆé”æˆã§çµ‚äº†ï¼‰ |

## ğŸ“ å‡ºåŠ›

- `submission.csv` - Kaggleæå‡ºç”¨ãƒ•ã‚¡ã‚¤ãƒ«
- `agent_result.json` - å®Ÿè¡Œçµæœã®ã‚µãƒãƒªãƒ¼
# kaggle-agent
